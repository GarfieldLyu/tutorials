{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification using Bert model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use hugging fase bert-base-uncased, you need to install it first (https://huggingface.co/transformers/model_doc/bert.html). \n",
    "The dataset here we use is the 2-class SST-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from pytorch_transformers import BertTokenizer, BertModel, AdamW, WarmupLinearSchedule\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the model architecture, which is simply the bert-base-uncased from huggingface, and a linear layer on top. The output dimension can either be 1 or 2 since it's a binary classification task, only with different  corresponding classification and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertParams:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'BERTModel.pt'\n",
    "        self.pretrained = 'bert-base-uncased'\n",
    "        self.max_sent_length = 254\n",
    "        self.hidden = 768  # to be checked\n",
    "\n",
    "\n",
    "class BertBase(nn.Module):\n",
    "    def __init__(self, out_dim):   # output dimension depends on the task.\n",
    "        super(BertBase, self).__init__()\n",
    "        self.Params = BertParams()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.Params.pretrained, cache_dir='/home/lyu/robustness/pretrained/bert/')   # download pretrained bert model to the cache dir.\n",
    "        self.bert = BertModel.from_pretrained(self.Params.pretrained, cache_dir='/home/lyu/robustness/pretrained/bert/')\n",
    "        self.bert.eval()\n",
    "        self.fc = nn.Linear(self.Params.hidden, out_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(input.shape) < 3 : \n",
    "           bert_out = self.bert(input)\n",
    "        else:   \n",
    "            # when the input is already embedding, input.shape == (batch, sequence_length, embedding_dimension)\n",
    "            # We seperate the embedding step only to simplify the gradient based feature methods, where you can directly feed embedding vectors as input.\n",
    "            bert_out = self.get_output_from_embedding(input)\n",
    "\n",
    "        encoded = bert_out[1]  # CLS  # use the pooled output from the last layer.\n",
    "        return self.fc(encoded)\n",
    "\n",
    "    def get_output_from_embedding(self, embedding, attention_mask=None, head_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(embedding.shape[0], embedding.shape[1]).to(embedding)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=next(self.bert.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "                head_mask = head_mask.expand(self.bert.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(\n",
    "                    -1)  # We can specify head_mask for each layer\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.bert.parameters()).dtype)  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.bert.config.num_hidden_layers\n",
    "        encoder_outputs = self.bert.encoder(embedding,\n",
    "                                             extended_attention_mask,\n",
    "                                             head_mask=head_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.bert.pooler(sequence_output)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertBase(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need the dataset to train this bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBinary(Dataset):   # prepare bert style instances.\n",
    "    def __init__(self, instances, tokenizer,  maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.instances = [[self.pad(sent[0], tokenizer, maxlen), sent[1]] for sent in instances]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.instances[idx][0]\n",
    "        y = self.instances[idx][1]\n",
    "        return x, y\n",
    "\n",
    "    def pad(self, instance, tokenizer, maxlen):    # pad value is  0 in bert tokenizer\n",
    "        padded = np.zeros((maxlen, ), dtype=np.int64)\n",
    "        if len(instance) > maxlen - 2:\n",
    "            instance = [tokenizer.cls_token] + instance[:maxlen -2] + [tokenizer.sep_token]\n",
    "            padded[:] = tokenizer.convert_tokens_to_ids(instance)\n",
    "        else:\n",
    "            instance = [tokenizer.cls_token] + instance + [tokenizer.sep_token]\n",
    "            padded[:len(instance)] = tokenizer.convert_tokens_to_ids(instance)\n",
    "\n",
    "        return padded\n",
    "\n",
    "    \n",
    "label2id_binary = {\"0\": 0, \"1\": 1}   # for SST y label.\n",
    "data_root = '/home/lyu/robustness/Datasets/'\n",
    "\n",
    "def load_splits_json(which_data):\n",
    "    def load_json(file):\n",
    "        with open(file, 'r')as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    print('Loading {} dataset...'.format(which_data))\n",
    "    directory = os.path.join(data_root, which_data+'data')\n",
    "    json_file = os.path.join(directory, which_data+'_input.json')\n",
    "    train_ids = os.path.join(directory, which_data+'_train_ids.json')\n",
    "    dev_ids = os.path.join(directory, which_data+'_dev_ids.json')\n",
    "    test_ids = os.path.join(directory, which_data+'_test_ids.json')\n",
    "    All_samples = load_json(json_file)\n",
    "    train_index = load_json(train_ids)\n",
    "    dev_index = load_json(dev_ids)\n",
    "    test_index = load_json(test_ids)\n",
    "    train_samples = [(All_samples[i]['en_defs'][0], All_samples[i]['label']) for i in train_index]\n",
    "    dev_samples = [(All_samples[i]['en_defs'][0], All_samples[i]['label']) for i in dev_index]\n",
    "    test_samples = [(All_samples[i]['en_defs'][0], All_samples[i]['label']) for i in test_index]\n",
    "    return train_samples, dev_samples, test_samples\n",
    "\n",
    "    \n",
    "def load_data_for_bert(which_data, tokenizer):   \n",
    "    if which_data == 'SST':\n",
    "        label2id = label2id_binary\n",
    "        MAX_LENGTH = 20\n",
    "    else:\n",
    "        raise ValueError('Datasets:  SST.')\n",
    "    print('Loading {} data for bert model...'.format(which_data))\n",
    "    train_samples, dev_samples, test_samples = load_splits_json(which_data)    # load SST data, which is json object of list of (text, label) tuple.\n",
    "    train_ids = [ [tokenizer.tokenize(sample[0]), label2id[sample[1]]] for sample in train_samples]\n",
    "    dev_ids = [ [tokenizer.tokenize(sample[0]), label2id[sample[1]]] for sample in dev_samples]\n",
    "    test_ids = [ [tokenizer.tokenize(sample[0]), label2id[sample[1]]] for sample in test_samples]\n",
    "\n",
    "    train_data = DataBinary(train_ids, tokenizer, MAX_LENGTH)\n",
    "    dev_data = DataBinary(dev_ids, tokenizer, MAX_LENGTH)\n",
    "    test_data = DataBinary(test_ids, tokenizer, MAX_LENGTH)\n",
    "    return train_data, dev_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST data for bert model...\n",
      "Loading SST dataset...\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data = load_data_for_bert('SST', model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look of the first training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128,\n",
      "        1011, 16603,  1997,  5053,  1998,  1996,  6841,  1998,  5687,\n",
      "        5469,   102]), 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)  # since we already set out_dim=1\n",
    "# criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y.unsqueeze(1).float()).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(train_data, optimizer, batch_size):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        iterator = iter(dataloader.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True))\n",
    "        for batch in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, label = batch[0].to(device), batch[1].to(device)\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, label.unsqueeze(1).float())\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    \n",
    "def evaluate(eval_data, batch_size):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    iterator = iter(dataloader.DataLoader(eval_data, batch_size=batch_size, shuffle=True, pin_memory=True))\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch[0].to(device), batch[1].to(device)\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, label.unsqueeze(1).float())\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [01:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64ebd04f5b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-341525264c78>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "epochs_num = 5  # let's only train for 5 epochs \n",
    "batch_size = 32\n",
    "optimizer = optim.Adam([{\"params\": model.bert.parameters(), \"lr\": lrmain}, {\"params\": model.fc.parameters(), \"lr\": lrlast}])\n",
    "best_valid_loss = float('inf')\n",
    "model_dir = '/home/lyu/robustness/SST/model/bert_sst.pt'\n",
    "\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(train_data, optimizer, batch_size)\n",
    "    valid_loss, valid_acc = evaluate(dev_data, batch_size)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_dir)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
